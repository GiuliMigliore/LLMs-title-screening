# The dataset
The dataset containing the titles and the human labels is a subset of a larger dataset from the [FORAS project] (https://asreview.nl/project/foras/). The dataset includes meta-data of papers potentially eligible for a systematic review on the trajectories of PTSD after traumatic events. The subset used for the current study consists of the records used for the two calibration sessions. Calibration Session 1 involved two screeners collaboratively reviewing 100 old records, 100 new records, and 100 randomly selected records, to refine the inclusion criteria. In Calibration Session 2, the same two screeners independently reviewed 100 old records, 100 new records, and 100 randomly selected records. During the second session, inter-rater reliability (IRR) was calculated, and any disagreements were discussed and resolved with the assistance of a third screener. The first batch of 300 records (Calibration set 1) is used to optimize the prompts and the second batch (Calibration set 2) is used for validation purposes. The dataset will be made public soon. Once available, the link will be provided in this section of the repository.

All the steps for the data preprocessing can be found in the `data_preprocessing.ipynb` notebook in the current folder.

The dataset containing the titles and the human labels is a subset of a larger dataset from the [FORAS project](https://asreview.nl/project/foras/). The dataset includes meta-data of  papers potentially eligible for a systematic review on the trajectories of PTSD after traumatic events. The subset used for the current study consists of the records used for the two calibration sessions. Calibration Session 1 involved two screeners collaboratively reviewing 100 old records, 100 new records, and 100 randomly selected records, to refine the inclusion criteria. In Calibration Session 2, the same two screeners independently reviewed 100 old records, 100 new records, and 100 randomly selected records. During the second session, inter-rater reliability (IRR) was calculated, and any disagreements were discussed and resolved with the assistance of a third screener. The first batch of 300 records (Calibration set 1) is used in the current study to optimize the prompts and the second batch (Calibration set 2) is used for validation purposes. The subset is available at the [Open Science Framework](NEEDS LINK). 

From the Calibration set 1, two new datasets are created containing only the relevant columns. One dataset includes the title labels and contains the following columns: "MID" (the paper identifier), "title" (the title of each scientific paper), "title_eligible" (the label given by a human expert screener based solely on the title). The other dataset includes the abstract labels and contains the following columns: "MID", "title", "TI-AB_final_label" (the label given by a human expert screener based on the paperâ€™s title and abstract).
There are 2 missing values in the "title_eligible" column and in the "TI-AB_final_label column", resulting in 298 records. 
The labels in the "title_eligible" column are originally stored as Y (relevant) or N (irrelevant), however, they are converted to 1 and 0, respectively. The labels in the  "TI-AB_final_label" column are already binarized.
Among the screened papers, 179 are labeled as relevant and 119 as irrelevant based on the title. There is no significant class imbalance, therefore it is not necessary to employ any balancing technique. 

The procedure implemented for Calibration Set 2 mirrors that of Calibration Set 1. 
Two new datasets are created for validation purposes: one containing labels based on titles, and the other containing labels based on abstracts. The column names remain consistent with those used in Calibration Set 1, as well as the total number of records, which is 300. However, there are 5 missing entries, resulting in 295 valid records. Finally, the "title_eligible" column is binarized.